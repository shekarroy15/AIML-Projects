{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "    \n",
    "In this notebook, we will build a neural network to classifiy the image based on the object present in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Info:\n",
    "    \n",
    "MNIST database of handwritten digits\n",
    "Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "\n",
    "Source: https://www.kaggle.com/warisali2/mnist-database-of-handwritten-digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced techniques for training neural networks:\n",
    "    \n",
    "Weight Initialization\n",
    "\n",
    "Nonlinearity (different Activation functions)\n",
    "\n",
    "Optimizers(different optimizers)\n",
    "\n",
    "Batch Normalization\n",
    "\n",
    "Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Libraries and dataset and do experimentation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADolJREFUeJzt3X2MXOV1x/HfyXq9jo1JvHVsHOJgxzgBYhqTjgzICFwhXKdCMqgCYkWRQ5M4LzgprStBraq4FancKiF1CUVamq1tifcEiv+gSZAVAVFhy+IQXuLwErMli7e7mA3YEOKX3dM/9m60MTvPrGfuzJ3d8/1I1szcc+/co4Hf3pl55t7H3F0A4nlP0Q0AKAbhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1LRG7my6tfkMzWrkLoFQfqu3dcQP20TWrSn8ZrZG0jZJLZL+3d23ptafoVk61y6uZZcAErp894TXrfptv5m1SLpF0qcknSVpnZmdVe3zAWisWj7zr5D0krvvc/cjku6StDaftgDUWy3hP1XSr8Y87s2W/R4z22Bm3WbWfVSHa9gdgDzVEv7xvlR41/nB7t7h7iV3L7WqrYbdAchTLeHvlbRwzOMPSdpfWzsAGqWW8D8haamZLTaz6ZI+LWlXPm0BqLeqh/rc/ZiZbZT0Q40M9XW6+3O5dQagrmoa53f3ByU9mFMvABqIn/cCQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVE2z9JpZj6RDkoYkHXP3Uh5NIT82Lf2fuOUDc+u6/+f/elHZ2tDM4eS2py0ZSNZnftWS9f+7aXrZ2p7S3cltDwy9nayfe++mZP30v3o8WW8GNYU/88fufiCH5wHQQLztB4KqNfwu6Udm9qSZbcijIQCNUevb/pXuvt/M5kl6yMx+4e6PjF0h+6OwQZJmaGaNuwOQl5qO/O6+P7sdkHS/pBXjrNPh7iV3L7WqrZbdAchR1eE3s1lmNnv0vqTVkp7NqzEA9VXL2/75ku43s9HnucPdf5BLVwDqrurwu/s+SZ/IsZcpq+XMpcm6t7Um6/sven+y/s555cek29+XHq9+9BPp8e4i/ddvZifr//SdNcl619l3lK29fPSd5LZb+y9J1j/4qCfrkwFDfUBQhB8IivADQRF+ICjCDwRF+IGg8jirL7yhVZ9M1m/afkuy/tHW8qeeTmVHfShZ/7ubP5esT3s7Pdx2/r0by9Zmv3osuW3bgfRQ4MzurmR9MuDIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc6fg7bn9yfrT/52YbL+0db+PNvJ1aa+85L1fW+lL/29fcn3ytbeHE6P08//1/9O1utp8p+wWxlHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IytwbN6J5srX7uXZxw/bXLAavPj9ZP7gmfXntlqdPStZ/9tWbT7inUTce+MNk/YmL0uP4Q2+8maz7+eWv7t7z9eSmWrzuZ+kV8C5dvlsHfTA9d3mGIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFVxnN/MOiVdKmnA3Zdly9ol3S1pkaQeSVe6+68r7SzqOH8lLXP/IFkfen0wWX/5jvJj9c9d2JncdsU/fi1Zn3dLcefU48TlPc6/XdLxE6FfL2m3uy+VtDt7DGASqRh+d39E0vGHnrWSdmT3d0i6LOe+ANRZtZ/557t7nyRlt/PyawlAI9T9Gn5mtkHSBkmaoZn13h2ACar2yN9vZgskKbsdKLeiu3e4e8ndS61qq3J3APJWbfh3SVqf3V8v6YF82gHQKBXDb2Z3SnpM0sfMrNfMPi9pq6RLzOxFSZdkjwFMIhU/87v7ujIlBuxzMnTg9Zq2P3pwetXbfvwzP0/WX7u1Jf0Ew0NV7xvF4hd+QFCEHwiK8ANBEX4gKMIPBEX4gaCYonsKOPO6F8rWrj47PSL7H6ftTtYvuuKaZH323Y8n62heHPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjG+aeA1DTZr3/lzOS2r+x6J1m//sadyfrfXHl5su4/fV/Z2sJvPJbcVg2cPj4ijvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTFKbrzxBTdzWfwz89P1m+/4ZvJ+uJpM6re98d3bkzWl97Wl6wf29dT9b6nqryn6AYwBRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAVx/nNrFPSpZIG3H1ZtmyLpC9Kei1bbbO7P1hpZ4zzTz6+cnmyfvLW3mT9zo/8sOp9n/HjLyTrH/v78tcxkKShF/dVve/JKu9x/u2S1oyz/Nvuvjz7VzH4AJpLxfC7+yOSBhvQC4AGquUz/0Yze9rMOs1sTm4dAWiIasN/q6QlkpZL6pP0rXIrmtkGM+s2s+6jOlzl7gDkrarwu3u/uw+5+7Ck2yStSKzb4e4ldy+1qq3aPgHkrKrwm9mCMQ8vl/RsPu0AaJSKl+42szslrZI018x6Jd0gaZWZLZfkknokfamOPQKoA87nR01a5s9L1vdfdXrZWtd125LbvqfCG9PPvLw6WX/zgteT9amI8/kBVET4gaAIPxAU4QeCIvxAUIQfCIqhPhTmnt70FN0zbXqy/hs/kqxf+rVryz/3/V3JbScrhvoAVET4gaAIPxAU4QeCIvxAUIQfCIrwA0FVPJ8fsQ1fkL509y+vSE/RvWx5T9lapXH8Sm4ePCdZn/lAd03PP9Vx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnn+KstCxZf+Hr6bH221buSNYvnJE+p74Wh/1osv744OL0Ewz35djN1MORHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2YLJe2UdIqkYUkd7r7NzNol3S1pkaQeSVe6+6/r12pc0xaflqz/8uoPlq1tuequ5LZ/dtKBqnrKw+b+UrL+8LbzkvU5O9LX/UfaRI78xyRtcvczJZ0n6RozO0vS9ZJ2u/tSSbuzxwAmiYrhd/c+d9+T3T8kaa+kUyWtlTT6868dki6rV5MA8ndCn/nNbJGkcyR1SZrv7n3SyB8ISfPybg5A/Uw4/GZ2kqTvS7rW3Q+ewHYbzKzbzLqP6nA1PQKogwmF38xaNRL82939vmxxv5ktyOoLJA2Mt627d7h7yd1LrWrLo2cAOagYfjMzSd+VtNfdbxpT2iVpfXZ/vaQH8m8PQL1M5JTelZI+K+kZM3sqW7ZZ0lZJ95jZ5yW9IumK+rQ4+U1b9OFk/c0/WpCsX/UPP0jWv/z++5L1etrUlx6Oe+zfyg/ntW//n+S2c4YZyquniuF3959IKjff98X5tgOgUfiFHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt09QdMWnFK2Ntg5K7ntVxY/nKyvm91fVU952PjqBcn6nlvTU3TP/d6zyXr7IcbqmxVHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKsw4/5E/SV8m+shfDibrm09/sGxt9XvfrqqnvPQPvVO2duGuTcltz/jbXyTr7W+kx+mHk1U0M478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUmHH+nsvSf+deOPveuu37ljeWJOvbHl6drNtQuSunjzjjxpfL1pb2dyW3HUpWMZVx5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMzd0yuYLZS0U9IpGjl9u8Pdt5nZFklflPRatupmdy9/0rukk63dzzVm9Qbqpct366APpn8YkpnIj3yOSdrk7nvMbLakJ83soaz2bXf/ZrWNAihOxfC7e5+kvuz+ITPbK+nUejcGoL5O6DO/mS2SdI6k0d+MbjSzp82s08zmlNlmg5l1m1n3UR2uqVkA+Zlw+M3sJEnfl3Stux+UdKukJZKWa+SdwbfG287dO9y95O6lVrXl0DKAPEwo/GbWqpHg3+7u90mSu/e7+5C7D0u6TdKK+rUJIG8Vw29mJum7kva6+01jli8Ys9rlktLTtQJoKhP5tn+lpM9KesbMnsqWbZa0zsyWS3JJPZK+VJcOAdTFRL7t/4mk8cYNk2P6AJobv/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfHS3bnuzOw1Sf87ZtFcSQca1sCJadbemrUvid6qlWdvp7n7ByayYkPD/66dm3W7e6mwBhKatbdm7Uuit2oV1Rtv+4GgCD8QVNHh7yh4/ynN2luz9iXRW7UK6a3Qz/wAilP0kR9AQQoJv5mtMbPnzewlM7u+iB7KMbMeM3vGzJ4ys+6Ce+k0swEze3bMsnYze8jMXsxux50mraDetpjZq9lr95SZ/WlBvS00sx+b2V4ze87M/iJbXuhrl+irkNet4W/7zaxF0guSLpHUK+kJSevc/ecNbaQMM+uRVHL3wseEzexCSW9J2unuy7Jl/yxp0N23Zn8457j7dU3S2xZJbxU9c3M2ocyCsTNLS7pM0udU4GuX6OtKFfC6FXHkXyHpJXff5+5HJN0laW0BfTQ9d39E0uBxi9dK2pHd36GR/3karkxvTcHd+9x9T3b/kKTRmaULfe0SfRWiiPCfKulXYx73qrmm/HZJPzKzJ81sQ9HNjGN+Nm366PTp8wru53gVZ25upONmlm6a166aGa/zVkT4x5v9p5mGHFa6+yclfUrSNdnbW0zMhGZubpRxZpZuCtXOeJ23IsLfK2nhmMcfkrS/gD7G5e77s9sBSfer+WYf7h+dJDW7HSi4n99pppmbx5tZWk3w2jXTjNdFhP8JSUvNbLGZTZf0aUm7CujjXcxsVvZFjMxslqTVar7Zh3dJWp/dXy/pgQJ7+T3NMnNzuZmlVfBr12wzXhfyI59sKONfJLVI6nT3bzS8iXGY2Uc0crSXRiYxvaPI3szsTkmrNHLWV7+kGyT9p6R7JH1Y0iuSrnD3hn/xVqa3VRp56/q7mZtHP2M3uLcLJD0q6RlJw9nizRr5fF3Ya5foa50KeN34hR8QFL/wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8D6+E2hIAP97kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  7\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[0])    # show first number in the dataset\n",
    "plt.show()\n",
    "print('Label: ', y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  7\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_test[0])    # show first number in the dataset\n",
    "plt.show()\n",
    "print('Label: ', y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping X data: (n, 28, 28) => (n, 784)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y data into categorical (one-hot encoding)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784) (60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive MLP model without any alterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50, input_shape = (784, )))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 1.7586 - acc: 0.3760\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 1.5209 - acc: 0.4368\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 66s 1ms/step - loss: 1.4157 - acc: 0.4699\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 1.2316 - acc: 0.5443\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 1.2443 - acc: 0.5231\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 1.1911 - acc: 0.5621\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 1.1653 - acc: 0.5733\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 1.1628 - acc: 0.5632\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 1.1377 - acc: 0.5782\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 67s 1ms/step - loss: 1.1357 - acc: 0.5851\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 2, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5529\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we will train Neural network using below techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Weight Initialization\n",
    "\n",
    "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem \n",
    "up to some degree\n",
    "\n",
    "Ref: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from now on, create a function to generate (return) models\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))     # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 2.5184 - acc: 0.1022\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.4409 - acc: 0.1022\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3877 - acc: 0.1022\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3501 - acc: 0.1023\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3254 - acc: 0.1036\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3107 - acc: 0.1083\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.3029 - acc: 0.1124\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2990 - acc: 0.1164\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2970 - acc: 0.1217\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2959 - acc: 0.1662\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2951 - acc: 0.1467\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 2.2944 - acc: 0.1168\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2937 - acc: 0.1143\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2931 - acc: 0.1131\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2925 - acc: 0.1128\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2919 - acc: 0.1128\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2914 - acc: 0.1127\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2908 - acc: 0.1127\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2902 - acc: 0.1131\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2896 - acc: 0.1127\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2890 - acc: 0.1128\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2884 - acc: 0.1135\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2878 - acc: 0.1128\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2872 - acc: 0.1137\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2866 - acc: 0.1132\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2860 - acc: 0.1145\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2854 - acc: 0.1144\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2848 - acc: 0.1133\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2842 - acc: 0.1146\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2837 - acc: 0.1153\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 2.2831 - acc: 0.1178\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2825 - acc: 0.1160\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2819 - acc: 0.1188\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2813 - acc: 0.1156\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2808 - acc: 0.1190\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2802 - acc: 0.1191\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2796 - acc: 0.1185\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2790 - acc: 0.1219\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2784 - acc: 0.1230\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2777 - acc: 0.1209\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2771 - acc: 0.1232\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2765 - acc: 0.1261\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2759 - acc: 0.1293\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2752 - acc: 0.1304\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2746 - acc: 0.1293\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2740 - acc: 0.1310\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2733 - acc: 0.1322\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2726 - acc: 0.1334\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2720 - acc: 0.1366\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2713 - acc: 0.1377\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 2.2706 - acc: 0.1451\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2699 - acc: 0.1445\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2692 - acc: 0.1510\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2684 - acc: 0.1576\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2677 - acc: 0.1571\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2669 - acc: 0.1528\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2662 - acc: 0.1641\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2654 - acc: 0.1587\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2646 - acc: 0.1769\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2638 - acc: 0.1814\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2630 - acc: 0.1807\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2622 - acc: 0.1881\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2614 - acc: 0.1858\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2605 - acc: 0.1957\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2597 - acc: 0.2117\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2588 - acc: 0.2081\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2579 - acc: 0.2187\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2570 - acc: 0.2258\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2561 - acc: 0.2261\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2551 - acc: 0.2327\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2542 - acc: 0.2345\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2532 - acc: 0.2458\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2522 - acc: 0.2510\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2511 - acc: 0.2541\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2501 - acc: 0.2780\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2490 - acc: 0.2683\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2479 - acc: 0.2722\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 2.2468 - acc: 0.2828\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 2.2457 - acc: 0.2744\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2445 - acc: 0.2993\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2433 - acc: 0.3040\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 2.2421 - acc: 0.3006\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2409 - acc: 0.3121\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2396 - acc: 0.3115\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2383 - acc: 0.3161\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2370 - acc: 0.3273\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2357 - acc: 0.3283\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2344 - acc: 0.3323\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2330 - acc: 0.3424\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 2.2315 - acc: 0.3394\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2301 - acc: 0.3548\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2286 - acc: 0.3614\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2271 - acc: 0.3573\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2255 - acc: 0.3559\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2239 - acc: 0.3679\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2222 - acc: 0.3754\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 2.2206 - acc: 0.3752\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2189 - acc: 0.3765\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2171 - acc: 0.3779\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 2.2153 - acc: 0.3888\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 66us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.3952\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nonlinearity (Activation function)\n",
    "\n",
    "Sigmoid functions suffer from gradient vanishing problem, making training slower\n",
    "\n",
    "There are many choices apart from sigmoid and tanh; try many of them!\n",
    "\n",
    "'relu' (rectified linear unit) is one of the most popular ones\n",
    "\n",
    "Ref: https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 1.1335 - acc: 0.7903\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.3403 - acc: 0.9016\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2647 - acc: 0.9216\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.2272 - acc: 0.9325\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.2015 - acc: 0.9397\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1822 - acc: 0.9455\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1699 - acc: 0.9489\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.1590 - acc: 0.9523\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.1488 - acc: 0.9544\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.1404 - acc: 0.9577\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 72us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9444\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Normalization\n",
    "\n",
    "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective\n",
    "\n",
    "Normalize each mini-batch before nonlinearity\n",
    "\n",
    "Ref: https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization layer is usually inserted after dense/convolution and before nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, )))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 1.5267 - acc: 0.532 - 10s 165us/step - loss: 1.5250 - acc: 0.5337\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.8726 - acc: 0.7823\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.6383 - acc: 0.8363\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.5240 - acc: 0.8614\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.4593 - acc: 0.8760\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.4124 - acc: 0.8859\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.3751 - acc: 0.8968\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3543 - acc: 0.9017\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.3301 - acc: 0.9063\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.3168 - acc: 0.9111\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.2991 - acc: 0.9158\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.2836 - acc: 0.9194\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.2779 - acc: 0.9208\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.2661 - acc: 0.9247\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.2617 - acc: 0.9256\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.2491 - acc: 0.9288\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.2386 - acc: 0.9311\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.2348 - acc: 0.9323\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.2261 - acc: 0.9345\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.2224 - acc: 0.9351\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 20, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 107us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9539\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.7666 - acc: 0.7634\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.4417 - acc: 0.8730\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.3661 - acc: 0.8956\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3252 - acc: 0.9065\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3009 - acc: 0.9150\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.2866 - acc: 0.9177\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.2692 - acc: 0.9236\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.2569 - acc: 0.9263\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.2443 - acc: 0.9304\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.2381 - acc: 0.9316\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 221us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9696\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
