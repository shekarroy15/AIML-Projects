{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"solutions_practice-exerecise_week4_nlp-aiml-online.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yllJR0PQpm4M","colab_type":"text"},"source":["# Text generation using a RNN"]},{"cell_type":"markdown","metadata":{"id":"2W6g03QsptwG","colab_type":"text"},"source":["Given a sequence of words from this data, train a model to predict the next word in the sequence. Longer sequences of text can be generated by calling the model repeatedly."]},{"cell_type":"markdown","metadata":{"id":"LjyyG4fM4ixQ","colab_type":"text"},"source":["Libraries used in this notebook along with their version:\n","\n","google\t2.0.3\n","\n","numpy\t1.18.3\n","\n","tensorflow\t2.2.0rc3\n","\n","sklearn 0.0"]},{"cell_type":"code","metadata":{"id":"2tF9iusgYl7s","colab_type":"code","outputId":"d0e82045-ecaf-43e0-c0cd-a1eef196ae53","executionInfo":{"status":"ok","timestamp":1588587818352,"user_tz":-330,"elapsed":2586,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow\n","tensorflow.__version__"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.0-rc3'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hbd3E0IuHwjz"},"source":["**Mount your Google Drive**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Dikrw1ylHtAL","outputId":"5041067a-6a07-4689-b57b-4e9b54816291","executionInfo":{"status":"ok","timestamp":1588587847089,"user_tz":-330,"elapsed":26185,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9-65VVnWO_xo","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/NLP Practice exercises test/W4/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fnIX_mLXHdxS"},"source":["### Import Keras and other libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0TmrQuvpHdxU","colab":{}},"source":["import glob\n","\n","from sklearn.utils import shuffle\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import backend"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zKBXQflGlPjG","colab_type":"text"},"source":["## Download data\n","Data is collected from http://www.gutenberg.org\n","\n","Go to this link to download the collected data\n","https://github.com/partoftheorigin/text-generation-datasets/tree/master/oscar_wilde"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s321mV4DHdxZ"},"source":["### Load the Oscar Wilde dataset\n","\n","Store all the \".txt\" file names in a list"]},{"cell_type":"code","metadata":{"id":"QMYW2stkTued","colab_type":"code","colab":{}},"source":["import zipfile"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIepZdCoTuuG","colab_type":"code","colab":{}},"source":["zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/NLP Practice exercises test/W4/data.zip\", 'r')\n","zip_ref.extractall(\"/content/drive/My Drive/NLP Practice exercises test/W4/tmp\")\n","zip_ref.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VUanlzNJHdxa","colab":{}},"source":["filelist = glob.glob(\"/content/drive/My Drive/NLP Practice exercises test/W4/tmp/data/*.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"glr4hv6uZkL-"},"source":["### Read the data\n","\n","Read contents of every file from the list and append the text in a new list"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zLrMMjrkRt9x","colab":{}},"source":["text_data = []\n","for file in filelist:\n","    with open(file, \"r\") as file:\n","      text_data.append(file.read())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jMX-Fu-GHdxj"},"source":["## Process the text\n","Initialize and fit the tokenizer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zQf1AV8wHdxl","colab":{}},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(text_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vpZ0A2-xHdxp"},"source":["### Vectorize the text\n","\n","Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping words to numbers, and another for numbers to words."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Nsq-rSPHdxq","colab":{}},"source":["word_idx = tokenizer.word_index #Last is the key\n","idx_word = tokenizer.index_word"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YMYdjx4aHdxu"},"source":["Get the word count for every word and also get the total number of words."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ioEZ2c21Hdxw","colab":{}},"source":["word_counts = tokenizer.word_counts\n","num_words = len(word_counts)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dWUBr9rHHdx0"},"source":["Convert text to sequence of numbers"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dwLl0BWKHdx2","colab":{}},"source":["sequences = tokenizer.texts_to_sequences(text_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GkpK8McUHdx6"},"source":["### Generate Features and Labels"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zxhQamjwHdx7","colab":{}},"source":["features = []\n","labels = []\n","\n","training_length = 50\n","# Iterate through the sequences of tokens\n","for seq in sequences:\n","    # Create multiple training examples from each sequence\n","    for i in range(training_length, training_length+300):\n","        # Extract the features and label\n","        extract = seq[i - training_length: i - training_length + 20]\n","\n","        # Set the features and label\n","        features.append(extract[:-1])\n","        labels.append(extract[-1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bbmsf23Bymwe"},"source":["### The prediction task"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wssHQ1oGymwe"},"source":["Given a word, or a sequence of words, what is the most probable next word? This is the task we're training the model to perform. The input to the model will be a sequence of words, and we train the model to predict the output—the following word at each time step.\n","\n","Since RNNs maintain an internal state that depends on the previously seen elements, given all the words computed until this moment, what is the next word?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T2bsVOl7HdyA"},"source":["### Generate training and testing data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j7-IsvynHdyB","colab":{}},"source":["from sklearn.utils import shuffle\n","import numpy as np\n","\n","features, labels = shuffle(features, labels, random_state=1)\n","\n","# Decide on number of samples for training\n","train_end = int(0.75 * len(labels))\n","\n","train_features = np.array(features[:train_end])\n","valid_features = np.array(features[train_end:])\n","\n","train_labels = labels[:train_end]\n","valid_labels = labels[train_end:]\n","\n","# Convert to arrays\n","X_train, X_valid = np.array(train_features), np.array(valid_features)\n","\n","# Using int8 for memory savings\n","y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n","y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n","\n","# One hot encoding of labels\n","for example_index, word_index in enumerate(train_labels):\n","    y_train[example_index, word_index] = 1\n","\n","for example_index, word_index in enumerate(valid_labels):\n","    y_valid[example_index, word_index] = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"juT1mZrUHdyE"},"source":["This is just to check the features and labels"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wkdmNbgjHdyF","outputId":"740e33e6-e552-46a3-ce2b-cf9fcd6fc5fa","executionInfo":{"status":"ok","timestamp":1588589555583,"user_tz":-330,"elapsed":946,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["for i, sequence in enumerate(X_train[:1]):\n","    text = []\n","    for idx in sequence:\n","        text.append(idx_word[idx])\n","        \n","    print('Features: ' + ' '.join(text)+'\\n')\n","    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Features: room in algernon's flat in half moon street the room is luxuriously and artistically furnished the sound of a\n","\n","Label: piano\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r6oUuElIMgVx"},"source":["## Build The Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m8gPwEjRzf-Z"},"source":["Use `keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n","\n","* `keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n","* `keras.layers.LSTM`: A type of RNN with size `units=rnn_units` (You can also use a GRU layer here.)\n","* `keras.layers.Dense`: The output layer, with `num_words` outputs."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GKpCQFZLHdyN","outputId":"9fbeae9a-60c6-4037-98c6-c5a738c365cb","executionInfo":{"status":"ok","timestamp":1588589582960,"user_tz":-330,"elapsed":1286,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["model = Sequential()\n","\n","# Embedding layer\n","model.add(\n","    Embedding(\n","        input_dim=num_words,\n","        output_dim=100,\n","        weights=None,\n","        trainable=True))\n","\n","# Recurrent layer\n","model.add(\n","    LSTM(\n","        64, return_sequences=False, dropout=0.1,\n","        recurrent_dropout=0.1))\n","\n","# Fully connected layer\n","model.add(Dense(64, activation='relu'))\n","\n","# Dropout for regularization\n","model.add(Dropout(0.5))\n","\n","# Output layer\n","model.add(Dense(num_words, activation='softmax'))\n","\n","# Compile the model\n","model.compile(\n","    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, None, 100)         3283800   \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 64)                42240     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 32838)             2134470   \n","=================================================================\n","Total params: 5,464,670\n","Trainable params: 5,464,670\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vL3tUp1UHdyS"},"source":["For each word the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next word."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LJL0Q0YPY6Ee"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6o84puBcHdyV","outputId":"c5b3ce71-5a4c-4212-a394-998743be77ff","executionInfo":{"status":"ok","timestamp":1588589598852,"user_tz":-330,"elapsed":14462,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["h = model.fit(X_train, y_train, epochs = 1, batch_size = 100, verbose = 1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["70/70 [==============================] - 11s 150ms/step - loss: 8.7434 - accuracy: 0.0553\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"82716QWAJrXG"},"source":["### Save Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_8MFkQgYJm-D","colab":{}},"source":["# save the model to file\n","model.save('./tmp/data/model_1000epochs.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4AYVKydVJv5C"},"source":["## If you have already trained the model and saved it, you can load a pretrained model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IqsQUz04J0GP","colab":{}},"source":["1\n","# load the model\n","2\n","model = load_model('./tmp/data/model_1000epochs.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZFe2Y0SJJ3Hb"},"source":["### Note: After loading the model run  model.fit()  to continue training form there, if required."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e9yLm_xnJ5JV","outputId":"b7ff0b8b-7ed6-4fa6-86cd-5aec5310cfc5","executionInfo":{"status":"ok","timestamp":1588597219424,"user_tz":-330,"elapsed":7549937,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit(X_train, y_train, batch_size=50, epochs=500)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/500\n","140/140 [==============================] - 16s 114ms/step - loss: 6.7567 - accuracy: 0.0598\n","Epoch 2/500\n","140/140 [==============================] - 15s 109ms/step - loss: 6.4419 - accuracy: 0.0595\n","Epoch 3/500\n","140/140 [==============================] - 15s 109ms/step - loss: 6.2613 - accuracy: 0.0632\n","Epoch 4/500\n","140/140 [==============================] - 15s 107ms/step - loss: 6.1193 - accuracy: 0.0638\n","Epoch 5/500\n","140/140 [==============================] - 15s 109ms/step - loss: 5.9700 - accuracy: 0.0654\n","Epoch 6/500\n","140/140 [==============================] - 15s 109ms/step - loss: 5.8448 - accuracy: 0.0698\n","Epoch 7/500\n","140/140 [==============================] - 15s 107ms/step - loss: 5.7453 - accuracy: 0.0727\n","Epoch 8/500\n","140/140 [==============================] - 15s 107ms/step - loss: 5.6328 - accuracy: 0.0766\n","Epoch 9/500\n","140/140 [==============================] - 15s 107ms/step - loss: 5.5397 - accuracy: 0.0840\n","Epoch 10/500\n","140/140 [==============================] - 15s 108ms/step - loss: 5.4604 - accuracy: 0.0912\n","Epoch 11/500\n","140/140 [==============================] - 15s 108ms/step - loss: 5.3404 - accuracy: 0.0972\n","Epoch 12/500\n","140/140 [==============================] - 15s 107ms/step - loss: 5.2293 - accuracy: 0.1071\n","Epoch 13/500\n","140/140 [==============================] - 15s 110ms/step - loss: 5.1005 - accuracy: 0.1211\n","Epoch 14/500\n","140/140 [==============================] - 15s 107ms/step - loss: 4.9635 - accuracy: 0.1368\n","Epoch 15/500\n","140/140 [==============================] - 15s 106ms/step - loss: 4.8109 - accuracy: 0.1547\n","Epoch 16/500\n","140/140 [==============================] - 15s 107ms/step - loss: 4.6410 - accuracy: 0.1867\n","Epoch 17/500\n","140/140 [==============================] - 15s 105ms/step - loss: 4.4830 - accuracy: 0.2089\n","Epoch 18/500\n","140/140 [==============================] - 15s 105ms/step - loss: 4.3372 - accuracy: 0.2244\n","Epoch 19/500\n","140/140 [==============================] - 15s 106ms/step - loss: 4.2352 - accuracy: 0.2347\n","Epoch 20/500\n","140/140 [==============================] - 15s 111ms/step - loss: 4.0827 - accuracy: 0.2644\n","Epoch 21/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.9934 - accuracy: 0.2720\n","Epoch 22/500\n","140/140 [==============================] - 15s 107ms/step - loss: 3.8777 - accuracy: 0.2899\n","Epoch 23/500\n","140/140 [==============================] - 15s 107ms/step - loss: 3.7967 - accuracy: 0.3029\n","Epoch 24/500\n","140/140 [==============================] - 15s 106ms/step - loss: 3.6859 - accuracy: 0.3250\n","Epoch 25/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.5973 - accuracy: 0.3341\n","Epoch 26/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.5151 - accuracy: 0.3395\n","Epoch 27/500\n","140/140 [==============================] - 15s 109ms/step - loss: 3.4338 - accuracy: 0.3472\n","Epoch 28/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.3508 - accuracy: 0.3586\n","Epoch 29/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.2832 - accuracy: 0.3663\n","Epoch 30/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.2162 - accuracy: 0.3755\n","Epoch 31/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.1525 - accuracy: 0.3818\n","Epoch 32/500\n","140/140 [==============================] - 15s 107ms/step - loss: 3.0767 - accuracy: 0.3920\n","Epoch 33/500\n","140/140 [==============================] - 15s 108ms/step - loss: 3.0021 - accuracy: 0.3991\n","Epoch 34/500\n","140/140 [==============================] - 15s 107ms/step - loss: 2.9466 - accuracy: 0.4063\n","Epoch 35/500\n","140/140 [==============================] - 15s 107ms/step - loss: 2.9067 - accuracy: 0.4108\n","Epoch 36/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.8323 - accuracy: 0.4234\n","Epoch 37/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.7947 - accuracy: 0.4258\n","Epoch 38/500\n","140/140 [==============================] - 15s 107ms/step - loss: 2.7318 - accuracy: 0.4304\n","Epoch 39/500\n","140/140 [==============================] - 15s 107ms/step - loss: 2.6701 - accuracy: 0.4443\n","Epoch 40/500\n","140/140 [==============================] - 15s 109ms/step - loss: 2.6273 - accuracy: 0.4466\n","Epoch 41/500\n","140/140 [==============================] - 15s 109ms/step - loss: 2.5696 - accuracy: 0.4505\n","Epoch 42/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.5251 - accuracy: 0.4579\n","Epoch 43/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.4445 - accuracy: 0.4730\n","Epoch 44/500\n","140/140 [==============================] - 15s 107ms/step - loss: 2.4011 - accuracy: 0.4796\n","Epoch 45/500\n","140/140 [==============================] - 15s 106ms/step - loss: 2.3603 - accuracy: 0.4814\n","Epoch 46/500\n","140/140 [==============================] - 15s 109ms/step - loss: 2.3019 - accuracy: 0.4882\n","Epoch 47/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.2633 - accuracy: 0.4928\n","Epoch 48/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.2172 - accuracy: 0.4935\n","Epoch 49/500\n","140/140 [==============================] - 15s 106ms/step - loss: 2.1688 - accuracy: 0.5016\n","Epoch 50/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.1203 - accuracy: 0.5118\n","Epoch 51/500\n","140/140 [==============================] - 15s 106ms/step - loss: 2.0959 - accuracy: 0.5125\n","Epoch 52/500\n","140/140 [==============================] - 15s 108ms/step - loss: 2.0603 - accuracy: 0.5170\n","Epoch 53/500\n","140/140 [==============================] - 15s 109ms/step - loss: 2.0255 - accuracy: 0.5214\n","Epoch 54/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.9627 - accuracy: 0.5381\n","Epoch 55/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.9532 - accuracy: 0.5308\n","Epoch 56/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.8991 - accuracy: 0.5418\n","Epoch 57/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.8742 - accuracy: 0.5531\n","Epoch 58/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.8430 - accuracy: 0.5640\n","Epoch 59/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.8237 - accuracy: 0.5561\n","Epoch 60/500\n","140/140 [==============================] - 16s 111ms/step - loss: 1.7696 - accuracy: 0.5620\n","Epoch 61/500\n","140/140 [==============================] - 15s 110ms/step - loss: 1.7588 - accuracy: 0.5656\n","Epoch 62/500\n","140/140 [==============================] - 15s 110ms/step - loss: 1.7468 - accuracy: 0.5654\n","Epoch 63/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.6949 - accuracy: 0.5741\n","Epoch 64/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.6817 - accuracy: 0.5819\n","Epoch 65/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.6633 - accuracy: 0.5848\n","Epoch 66/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.6339 - accuracy: 0.5935\n","Epoch 67/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.5973 - accuracy: 0.5963\n","Epoch 68/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.6171 - accuracy: 0.5903\n","Epoch 69/500\n","140/140 [==============================] - 15s 104ms/step - loss: 1.5627 - accuracy: 0.6056\n","Epoch 70/500\n","140/140 [==============================] - 15s 105ms/step - loss: 1.5285 - accuracy: 0.6087\n","Epoch 71/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.5391 - accuracy: 0.6056\n","Epoch 72/500\n","140/140 [==============================] - 15s 105ms/step - loss: 1.4988 - accuracy: 0.6120\n","Epoch 73/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.4954 - accuracy: 0.6151\n","Epoch 74/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.4829 - accuracy: 0.6194\n","Epoch 75/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.4821 - accuracy: 0.6229\n","Epoch 76/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.4342 - accuracy: 0.6255\n","Epoch 77/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.4253 - accuracy: 0.6267\n","Epoch 78/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.3873 - accuracy: 0.6328\n","Epoch 79/500\n","140/140 [==============================] - 15s 105ms/step - loss: 1.4082 - accuracy: 0.6275\n","Epoch 80/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.3712 - accuracy: 0.6370\n","Epoch 81/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.3760 - accuracy: 0.6416\n","Epoch 82/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.3653 - accuracy: 0.6315\n","Epoch 83/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.3297 - accuracy: 0.6499\n","Epoch 84/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.3064 - accuracy: 0.6506\n","Epoch 85/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.3207 - accuracy: 0.6430\n","Epoch 86/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.2677 - accuracy: 0.6576\n","Epoch 87/500\n","140/140 [==============================] - 15s 105ms/step - loss: 1.2871 - accuracy: 0.6538\n","Epoch 88/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.2789 - accuracy: 0.6552\n","Epoch 89/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.2698 - accuracy: 0.6569\n","Epoch 90/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.2200 - accuracy: 0.6688\n","Epoch 91/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.2038 - accuracy: 0.6777\n","Epoch 92/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.2196 - accuracy: 0.6674\n","Epoch 93/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.1900 - accuracy: 0.6740\n","Epoch 94/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.1990 - accuracy: 0.6700\n","Epoch 95/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.1926 - accuracy: 0.6804\n","Epoch 96/500\n","140/140 [==============================] - 15s 110ms/step - loss: 1.1820 - accuracy: 0.6723\n","Epoch 97/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.1740 - accuracy: 0.6780\n","Epoch 98/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.1752 - accuracy: 0.6781\n","Epoch 99/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.1534 - accuracy: 0.6850\n","Epoch 100/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.1112 - accuracy: 0.6919\n","Epoch 101/500\n","140/140 [==============================] - 15s 110ms/step - loss: 1.1419 - accuracy: 0.6867\n","Epoch 102/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.1259 - accuracy: 0.6869\n","Epoch 103/500\n","140/140 [==============================] - 15s 109ms/step - loss: 1.0937 - accuracy: 0.6972\n","Epoch 104/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.1094 - accuracy: 0.6959\n","Epoch 105/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.1051 - accuracy: 0.6919\n","Epoch 106/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.0862 - accuracy: 0.7057\n","Epoch 107/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.0655 - accuracy: 0.7015\n","Epoch 108/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.0721 - accuracy: 0.7012\n","Epoch 109/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.0649 - accuracy: 0.7075\n","Epoch 110/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.0578 - accuracy: 0.7014\n","Epoch 111/500\n","140/140 [==============================] - 15s 105ms/step - loss: 1.0448 - accuracy: 0.7064\n","Epoch 112/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.0479 - accuracy: 0.7039\n","Epoch 113/500\n","140/140 [==============================] - 15s 107ms/step - loss: 1.0080 - accuracy: 0.7151\n","Epoch 114/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.0133 - accuracy: 0.7193\n","Epoch 115/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.0251 - accuracy: 0.7095\n","Epoch 116/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.0187 - accuracy: 0.7101\n","Epoch 117/500\n","140/140 [==============================] - 15s 106ms/step - loss: 1.0008 - accuracy: 0.7203\n","Epoch 118/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.9914 - accuracy: 0.7203\n","Epoch 119/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.9857 - accuracy: 0.7203\n","Epoch 120/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.9792 - accuracy: 0.7265\n","Epoch 121/500\n","140/140 [==============================] - 15s 108ms/step - loss: 1.0027 - accuracy: 0.7104\n","Epoch 122/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.9544 - accuracy: 0.7256\n","Epoch 123/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.9532 - accuracy: 0.7275\n","Epoch 124/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.9420 - accuracy: 0.7273\n","Epoch 125/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.9452 - accuracy: 0.7260\n","Epoch 126/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.9508 - accuracy: 0.7290\n","Epoch 127/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.9532 - accuracy: 0.7257\n","Epoch 128/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.9534 - accuracy: 0.7265\n","Epoch 129/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.9198 - accuracy: 0.7355\n","Epoch 130/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8983 - accuracy: 0.7379\n","Epoch 131/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.9367 - accuracy: 0.7305\n","Epoch 132/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.9178 - accuracy: 0.7343\n","Epoch 133/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.9132 - accuracy: 0.7316\n","Epoch 134/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.9025 - accuracy: 0.7422\n","Epoch 135/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.9031 - accuracy: 0.7369\n","Epoch 136/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.9062 - accuracy: 0.7361\n","Epoch 137/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.8790 - accuracy: 0.7498\n","Epoch 138/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.8859 - accuracy: 0.7368\n","Epoch 139/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.8623 - accuracy: 0.7487\n","Epoch 140/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8699 - accuracy: 0.7464\n","Epoch 141/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.8787 - accuracy: 0.7438\n","Epoch 142/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8544 - accuracy: 0.7497\n","Epoch 143/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8368 - accuracy: 0.7520\n","Epoch 144/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.8532 - accuracy: 0.7491\n","Epoch 145/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.8416 - accuracy: 0.7495\n","Epoch 146/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8307 - accuracy: 0.7580\n","Epoch 147/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8204 - accuracy: 0.7629\n","Epoch 148/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.8319 - accuracy: 0.7533\n","Epoch 149/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.8109 - accuracy: 0.7617\n","Epoch 150/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8012 - accuracy: 0.7620\n","Epoch 151/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7919 - accuracy: 0.7652\n","Epoch 152/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7955 - accuracy: 0.7654\n","Epoch 153/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7938 - accuracy: 0.7659\n","Epoch 154/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.8038 - accuracy: 0.7619\n","Epoch 155/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.7998 - accuracy: 0.7647\n","Epoch 156/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.8067 - accuracy: 0.7606\n","Epoch 157/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.8144 - accuracy: 0.7630\n","Epoch 158/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7774 - accuracy: 0.7739\n","Epoch 159/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7671 - accuracy: 0.7788\n","Epoch 160/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7689 - accuracy: 0.7726\n","Epoch 161/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.7664 - accuracy: 0.7723\n","Epoch 162/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7880 - accuracy: 0.7693\n","Epoch 163/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7677 - accuracy: 0.7745\n","Epoch 164/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7783 - accuracy: 0.7649\n","Epoch 165/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.7372 - accuracy: 0.7835\n","Epoch 166/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7407 - accuracy: 0.7782\n","Epoch 167/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7272 - accuracy: 0.7870\n","Epoch 168/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7506 - accuracy: 0.7700\n","Epoch 169/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7597 - accuracy: 0.7743\n","Epoch 170/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7447 - accuracy: 0.7766\n","Epoch 171/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7403 - accuracy: 0.7765\n","Epoch 172/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.7149 - accuracy: 0.7809\n","Epoch 173/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7186 - accuracy: 0.7854\n","Epoch 174/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7202 - accuracy: 0.7868\n","Epoch 175/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.7206 - accuracy: 0.7819\n","Epoch 176/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.7289 - accuracy: 0.7796\n","Epoch 177/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.7139 - accuracy: 0.7867\n","Epoch 178/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6954 - accuracy: 0.7888\n","Epoch 179/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6971 - accuracy: 0.7882\n","Epoch 180/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.7050 - accuracy: 0.7828\n","Epoch 181/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.7092 - accuracy: 0.7905\n","Epoch 182/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6979 - accuracy: 0.7898\n","Epoch 183/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.7108 - accuracy: 0.7851\n","Epoch 184/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6696 - accuracy: 0.8000\n","Epoch 185/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.7034 - accuracy: 0.7884\n","Epoch 186/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6976 - accuracy: 0.7888\n","Epoch 187/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6978 - accuracy: 0.7877\n","Epoch 188/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6710 - accuracy: 0.7943\n","Epoch 189/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6851 - accuracy: 0.7960\n","Epoch 190/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6790 - accuracy: 0.7914\n","Epoch 191/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6877 - accuracy: 0.7920\n","Epoch 192/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.6723 - accuracy: 0.7984\n","Epoch 193/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6556 - accuracy: 0.8033\n","Epoch 194/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6494 - accuracy: 0.8004\n","Epoch 195/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6669 - accuracy: 0.7983\n","Epoch 196/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6553 - accuracy: 0.8047\n","Epoch 197/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6585 - accuracy: 0.8013\n","Epoch 198/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6608 - accuracy: 0.8001\n","Epoch 199/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6595 - accuracy: 0.8024\n","Epoch 200/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.6664 - accuracy: 0.7957\n","Epoch 201/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6388 - accuracy: 0.8043\n","Epoch 202/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6485 - accuracy: 0.8054\n","Epoch 203/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6323 - accuracy: 0.8096\n","Epoch 204/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6341 - accuracy: 0.8066\n","Epoch 205/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6378 - accuracy: 0.8037\n","Epoch 206/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6418 - accuracy: 0.8075\n","Epoch 207/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6311 - accuracy: 0.8053\n","Epoch 208/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6257 - accuracy: 0.8125\n","Epoch 209/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6184 - accuracy: 0.8106\n","Epoch 210/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6200 - accuracy: 0.8109\n","Epoch 211/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6141 - accuracy: 0.8103\n","Epoch 212/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6224 - accuracy: 0.8085\n","Epoch 213/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6263 - accuracy: 0.8126\n","Epoch 214/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6165 - accuracy: 0.8132\n","Epoch 215/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6148 - accuracy: 0.8115\n","Epoch 216/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6277 - accuracy: 0.8057\n","Epoch 217/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6162 - accuracy: 0.8132\n","Epoch 218/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.6111 - accuracy: 0.8184\n","Epoch 219/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6078 - accuracy: 0.8173\n","Epoch 220/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5912 - accuracy: 0.8195\n","Epoch 221/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6036 - accuracy: 0.8195\n","Epoch 222/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.5909 - accuracy: 0.8186\n","Epoch 223/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5972 - accuracy: 0.8132\n","Epoch 224/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.6128 - accuracy: 0.8112\n","Epoch 225/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.6057 - accuracy: 0.8179\n","Epoch 226/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5617 - accuracy: 0.8252\n","Epoch 227/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5731 - accuracy: 0.8268\n","Epoch 228/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5901 - accuracy: 0.8173\n","Epoch 229/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.6002 - accuracy: 0.8159\n","Epoch 230/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5802 - accuracy: 0.8205\n","Epoch 231/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5804 - accuracy: 0.8239\n","Epoch 232/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5614 - accuracy: 0.8249\n","Epoch 233/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5842 - accuracy: 0.8169\n","Epoch 234/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5659 - accuracy: 0.8270\n","Epoch 235/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5973 - accuracy: 0.8192\n","Epoch 236/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5830 - accuracy: 0.8152\n","Epoch 237/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5755 - accuracy: 0.8232\n","Epoch 238/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5478 - accuracy: 0.8305\n","Epoch 239/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5451 - accuracy: 0.8341\n","Epoch 240/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5589 - accuracy: 0.8271\n","Epoch 241/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5671 - accuracy: 0.8281\n","Epoch 242/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5682 - accuracy: 0.8228\n","Epoch 243/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5605 - accuracy: 0.8267\n","Epoch 244/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5596 - accuracy: 0.8271\n","Epoch 245/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5593 - accuracy: 0.8298\n","Epoch 246/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5621 - accuracy: 0.8278\n","Epoch 247/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5732 - accuracy: 0.8249\n","Epoch 248/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5571 - accuracy: 0.8278\n","Epoch 249/500\n","140/140 [==============================] - 16s 117ms/step - loss: 0.5284 - accuracy: 0.8367\n","Epoch 250/500\n","140/140 [==============================] - 18s 130ms/step - loss: 0.5574 - accuracy: 0.8285\n","Epoch 251/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5448 - accuracy: 0.8338\n","Epoch 252/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5590 - accuracy: 0.8244\n","Epoch 253/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5589 - accuracy: 0.8318\n","Epoch 254/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5351 - accuracy: 0.8314\n","Epoch 255/500\n","140/140 [==============================] - 16s 117ms/step - loss: 0.5591 - accuracy: 0.8295\n","Epoch 256/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.5425 - accuracy: 0.8384\n","Epoch 257/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5283 - accuracy: 0.8343\n","Epoch 258/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5199 - accuracy: 0.8397\n","Epoch 259/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5339 - accuracy: 0.8311\n","Epoch 260/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5481 - accuracy: 0.8282\n","Epoch 261/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5304 - accuracy: 0.8364\n","Epoch 262/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5377 - accuracy: 0.8320\n","Epoch 263/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5414 - accuracy: 0.8252\n","Epoch 264/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5257 - accuracy: 0.8403\n","Epoch 265/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5361 - accuracy: 0.8340\n","Epoch 266/500\n","140/140 [==============================] - 16s 112ms/step - loss: 0.5349 - accuracy: 0.8348\n","Epoch 267/500\n","140/140 [==============================] - 16s 113ms/step - loss: 0.5157 - accuracy: 0.8399\n","Epoch 268/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5248 - accuracy: 0.8377\n","Epoch 269/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5119 - accuracy: 0.8401\n","Epoch 270/500\n","140/140 [==============================] - 16s 114ms/step - loss: 0.5160 - accuracy: 0.8410\n","Epoch 271/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5133 - accuracy: 0.8432\n","Epoch 272/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5317 - accuracy: 0.8376\n","Epoch 273/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5036 - accuracy: 0.8384\n","Epoch 274/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.5253 - accuracy: 0.8364\n","Epoch 275/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.5263 - accuracy: 0.8350\n","Epoch 276/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5301 - accuracy: 0.8334\n","Epoch 277/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5083 - accuracy: 0.8416\n","Epoch 278/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5250 - accuracy: 0.8341\n","Epoch 279/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5077 - accuracy: 0.8447\n","Epoch 280/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.4967 - accuracy: 0.8465\n","Epoch 281/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.5428 - accuracy: 0.8350\n","Epoch 282/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5135 - accuracy: 0.8396\n","Epoch 283/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4931 - accuracy: 0.8476\n","Epoch 284/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5162 - accuracy: 0.8394\n","Epoch 285/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5096 - accuracy: 0.8439\n","Epoch 286/500\n","140/140 [==============================] - 16s 112ms/step - loss: 0.5104 - accuracy: 0.8404\n","Epoch 287/500\n","140/140 [==============================] - 16s 112ms/step - loss: 0.5231 - accuracy: 0.8389\n","Epoch 288/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.4994 - accuracy: 0.8442\n","Epoch 289/500\n","140/140 [==============================] - 21s 151ms/step - loss: 0.4862 - accuracy: 0.8483\n","Epoch 290/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5002 - accuracy: 0.8386\n","Epoch 291/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4736 - accuracy: 0.8502\n","Epoch 292/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4746 - accuracy: 0.8529\n","Epoch 293/500\n","140/140 [==============================] - 16s 112ms/step - loss: 0.5022 - accuracy: 0.8475\n","Epoch 294/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.5042 - accuracy: 0.8422\n","Epoch 295/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.4731 - accuracy: 0.8538\n","Epoch 296/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.5010 - accuracy: 0.8434\n","Epoch 297/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4811 - accuracy: 0.8466\n","Epoch 298/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4823 - accuracy: 0.8515\n","Epoch 299/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4849 - accuracy: 0.8476\n","Epoch 300/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4750 - accuracy: 0.8549\n","Epoch 301/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4994 - accuracy: 0.8457\n","Epoch 302/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4811 - accuracy: 0.8492\n","Epoch 303/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.5001 - accuracy: 0.8462\n","Epoch 304/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4868 - accuracy: 0.8433\n","Epoch 305/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4986 - accuracy: 0.8475\n","Epoch 306/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4938 - accuracy: 0.8486\n","Epoch 307/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4947 - accuracy: 0.8460\n","Epoch 308/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4809 - accuracy: 0.8483\n","Epoch 309/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4489 - accuracy: 0.8569\n","Epoch 310/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4869 - accuracy: 0.8470\n","Epoch 311/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.4811 - accuracy: 0.8508\n","Epoch 312/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4961 - accuracy: 0.8460\n","Epoch 313/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4694 - accuracy: 0.8510\n","Epoch 314/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4738 - accuracy: 0.8510\n","Epoch 315/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4665 - accuracy: 0.8562\n","Epoch 316/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4904 - accuracy: 0.8482\n","Epoch 317/500\n","140/140 [==============================] - 16s 111ms/step - loss: 0.4913 - accuracy: 0.8489\n","Epoch 318/500\n","140/140 [==============================] - 15s 110ms/step - loss: 0.4594 - accuracy: 0.8565\n","Epoch 319/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4634 - accuracy: 0.8503\n","Epoch 320/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4485 - accuracy: 0.8608\n","Epoch 321/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4728 - accuracy: 0.8555\n","Epoch 322/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4677 - accuracy: 0.8495\n","Epoch 323/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4720 - accuracy: 0.8496\n","Epoch 324/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4655 - accuracy: 0.8500\n","Epoch 325/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4535 - accuracy: 0.8612\n","Epoch 326/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4474 - accuracy: 0.8563\n","Epoch 327/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4579 - accuracy: 0.8596\n","Epoch 328/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4474 - accuracy: 0.8601\n","Epoch 329/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4539 - accuracy: 0.8594\n","Epoch 330/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4599 - accuracy: 0.8512\n","Epoch 331/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4771 - accuracy: 0.8520\n","Epoch 332/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4386 - accuracy: 0.8634\n","Epoch 333/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4517 - accuracy: 0.8563\n","Epoch 334/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4683 - accuracy: 0.8529\n","Epoch 335/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4367 - accuracy: 0.8651\n","Epoch 336/500\n","140/140 [==============================] - 15s 109ms/step - loss: 0.4392 - accuracy: 0.8621\n","Epoch 337/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4525 - accuracy: 0.8552\n","Epoch 338/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4654 - accuracy: 0.8545\n","Epoch 339/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4771 - accuracy: 0.8503\n","Epoch 340/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4603 - accuracy: 0.8569\n","Epoch 341/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4564 - accuracy: 0.8575\n","Epoch 342/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4495 - accuracy: 0.8575\n","Epoch 343/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4338 - accuracy: 0.8614\n","Epoch 344/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4495 - accuracy: 0.8615\n","Epoch 345/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4523 - accuracy: 0.8589\n","Epoch 346/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4517 - accuracy: 0.8601\n","Epoch 347/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4689 - accuracy: 0.8530\n","Epoch 348/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4609 - accuracy: 0.8516\n","Epoch 349/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4542 - accuracy: 0.8513\n","Epoch 350/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4284 - accuracy: 0.8648\n","Epoch 351/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4516 - accuracy: 0.8549\n","Epoch 352/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4415 - accuracy: 0.8609\n","Epoch 353/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4400 - accuracy: 0.8616\n","Epoch 354/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4400 - accuracy: 0.8641\n","Epoch 355/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4348 - accuracy: 0.8615\n","Epoch 356/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4366 - accuracy: 0.8596\n","Epoch 357/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4235 - accuracy: 0.8628\n","Epoch 358/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.4276 - accuracy: 0.8675\n","Epoch 359/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4369 - accuracy: 0.8655\n","Epoch 360/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.4352 - accuracy: 0.8595\n","Epoch 361/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4351 - accuracy: 0.8608\n","Epoch 362/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4413 - accuracy: 0.8566\n","Epoch 363/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4245 - accuracy: 0.8670\n","Epoch 364/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4398 - accuracy: 0.8602\n","Epoch 365/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4358 - accuracy: 0.8618\n","Epoch 366/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4293 - accuracy: 0.8652\n","Epoch 367/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4258 - accuracy: 0.8654\n","Epoch 368/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4109 - accuracy: 0.8703\n","Epoch 369/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4434 - accuracy: 0.8602\n","Epoch 370/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4349 - accuracy: 0.8664\n","Epoch 371/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4386 - accuracy: 0.8612\n","Epoch 372/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4172 - accuracy: 0.8659\n","Epoch 373/500\n","140/140 [==============================] - 15s 108ms/step - loss: 0.4192 - accuracy: 0.8651\n","Epoch 374/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4108 - accuracy: 0.8692\n","Epoch 375/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4193 - accuracy: 0.8678\n","Epoch 376/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4093 - accuracy: 0.8674\n","Epoch 377/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4093 - accuracy: 0.8682\n","Epoch 378/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4241 - accuracy: 0.8677\n","Epoch 379/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4218 - accuracy: 0.8677\n","Epoch 380/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4079 - accuracy: 0.8740\n","Epoch 381/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4307 - accuracy: 0.8628\n","Epoch 382/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4075 - accuracy: 0.8690\n","Epoch 383/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4215 - accuracy: 0.8672\n","Epoch 384/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4017 - accuracy: 0.8727\n","Epoch 385/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4174 - accuracy: 0.8657\n","Epoch 386/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4088 - accuracy: 0.8731\n","Epoch 387/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.4049 - accuracy: 0.8690\n","Epoch 388/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4147 - accuracy: 0.8654\n","Epoch 389/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4084 - accuracy: 0.8718\n","Epoch 390/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.3900 - accuracy: 0.8778\n","Epoch 391/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4124 - accuracy: 0.8725\n","Epoch 392/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4049 - accuracy: 0.8721\n","Epoch 393/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.3949 - accuracy: 0.8774\n","Epoch 394/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3842 - accuracy: 0.8809\n","Epoch 395/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3942 - accuracy: 0.8763\n","Epoch 396/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.4032 - accuracy: 0.8753\n","Epoch 397/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4206 - accuracy: 0.8657\n","Epoch 398/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4078 - accuracy: 0.8718\n","Epoch 399/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.4005 - accuracy: 0.8724\n","Epoch 400/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4224 - accuracy: 0.8644\n","Epoch 401/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3819 - accuracy: 0.8789\n","Epoch 402/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4107 - accuracy: 0.8680\n","Epoch 403/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4096 - accuracy: 0.8730\n","Epoch 404/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.4119 - accuracy: 0.8678\n","Epoch 405/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3994 - accuracy: 0.8731\n","Epoch 406/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4022 - accuracy: 0.8724\n","Epoch 407/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4106 - accuracy: 0.8718\n","Epoch 408/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4123 - accuracy: 0.8705\n","Epoch 409/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4087 - accuracy: 0.8717\n","Epoch 410/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3898 - accuracy: 0.8766\n","Epoch 411/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4096 - accuracy: 0.8700\n","Epoch 412/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4063 - accuracy: 0.8705\n","Epoch 413/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3784 - accuracy: 0.8794\n","Epoch 414/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.4110 - accuracy: 0.8731\n","Epoch 415/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3932 - accuracy: 0.8733\n","Epoch 416/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3871 - accuracy: 0.8774\n","Epoch 417/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3948 - accuracy: 0.8744\n","Epoch 418/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3949 - accuracy: 0.8764\n","Epoch 419/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3931 - accuracy: 0.8777\n","Epoch 420/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3929 - accuracy: 0.8758\n","Epoch 421/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.4034 - accuracy: 0.8758\n","Epoch 422/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3921 - accuracy: 0.8810\n","Epoch 423/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3845 - accuracy: 0.8756\n","Epoch 424/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3851 - accuracy: 0.8829\n","Epoch 425/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3758 - accuracy: 0.8807\n","Epoch 426/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3817 - accuracy: 0.8786\n","Epoch 427/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3875 - accuracy: 0.8789\n","Epoch 428/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3812 - accuracy: 0.8787\n","Epoch 429/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3740 - accuracy: 0.8791\n","Epoch 430/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3849 - accuracy: 0.8800\n","Epoch 431/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3878 - accuracy: 0.8776\n","Epoch 432/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3717 - accuracy: 0.8824\n","Epoch 433/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3968 - accuracy: 0.8717\n","Epoch 434/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3940 - accuracy: 0.8781\n","Epoch 435/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3742 - accuracy: 0.8811\n","Epoch 436/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3812 - accuracy: 0.8750\n","Epoch 437/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3916 - accuracy: 0.8758\n","Epoch 438/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3816 - accuracy: 0.8766\n","Epoch 439/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3827 - accuracy: 0.8784\n","Epoch 440/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3781 - accuracy: 0.8783\n","Epoch 441/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3777 - accuracy: 0.8809\n","Epoch 442/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3782 - accuracy: 0.8797\n","Epoch 443/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3929 - accuracy: 0.8770\n","Epoch 444/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3849 - accuracy: 0.8810\n","Epoch 445/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3773 - accuracy: 0.8780\n","Epoch 446/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3835 - accuracy: 0.8800\n","Epoch 447/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3750 - accuracy: 0.8783\n","Epoch 448/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3584 - accuracy: 0.8883\n","Epoch 449/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3837 - accuracy: 0.8776\n","Epoch 450/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3673 - accuracy: 0.8813\n","Epoch 451/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3556 - accuracy: 0.8872\n","Epoch 452/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3755 - accuracy: 0.8830\n","Epoch 453/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3839 - accuracy: 0.8800\n","Epoch 454/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3695 - accuracy: 0.8833\n","Epoch 455/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3746 - accuracy: 0.8766\n","Epoch 456/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3649 - accuracy: 0.8837\n","Epoch 457/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3585 - accuracy: 0.8799\n","Epoch 458/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3774 - accuracy: 0.8784\n","Epoch 459/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3620 - accuracy: 0.8875\n","Epoch 460/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3636 - accuracy: 0.8857\n","Epoch 461/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3666 - accuracy: 0.8826\n","Epoch 462/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3707 - accuracy: 0.8794\n","Epoch 463/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3860 - accuracy: 0.8743\n","Epoch 464/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3370 - accuracy: 0.8941\n","Epoch 465/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3752 - accuracy: 0.8786\n","Epoch 466/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3782 - accuracy: 0.8826\n","Epoch 467/500\n","140/140 [==============================] - 14s 104ms/step - loss: 0.3638 - accuracy: 0.8860\n","Epoch 468/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3484 - accuracy: 0.8849\n","Epoch 469/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3633 - accuracy: 0.8833\n","Epoch 470/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3675 - accuracy: 0.8850\n","Epoch 471/500\n","140/140 [==============================] - 14s 104ms/step - loss: 0.3954 - accuracy: 0.8734\n","Epoch 472/500\n","140/140 [==============================] - 17s 122ms/step - loss: 0.3648 - accuracy: 0.8810\n","Epoch 473/500\n","140/140 [==============================] - 16s 118ms/step - loss: 0.3525 - accuracy: 0.8885\n","Epoch 474/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3547 - accuracy: 0.8870\n","Epoch 475/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3587 - accuracy: 0.8860\n","Epoch 476/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3725 - accuracy: 0.8826\n","Epoch 477/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3697 - accuracy: 0.8811\n","Epoch 478/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3592 - accuracy: 0.8834\n","Epoch 479/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3496 - accuracy: 0.8876\n","Epoch 480/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3585 - accuracy: 0.8862\n","Epoch 481/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3565 - accuracy: 0.8873\n","Epoch 482/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3434 - accuracy: 0.8906\n","Epoch 483/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3551 - accuracy: 0.8866\n","Epoch 484/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3662 - accuracy: 0.8860\n","Epoch 485/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3728 - accuracy: 0.8849\n","Epoch 486/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3580 - accuracy: 0.8827\n","Epoch 487/500\n","140/140 [==============================] - 14s 102ms/step - loss: 0.3709 - accuracy: 0.8801\n","Epoch 488/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3517 - accuracy: 0.8906\n","Epoch 489/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3358 - accuracy: 0.8920\n","Epoch 490/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3455 - accuracy: 0.8913\n","Epoch 491/500\n","140/140 [==============================] - 14s 103ms/step - loss: 0.3537 - accuracy: 0.8897\n","Epoch 492/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3523 - accuracy: 0.8906\n","Epoch 493/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3484 - accuracy: 0.8886\n","Epoch 494/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3725 - accuracy: 0.8826\n","Epoch 495/500\n","140/140 [==============================] - 15s 107ms/step - loss: 0.3732 - accuracy: 0.8823\n","Epoch 496/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3506 - accuracy: 0.8859\n","Epoch 497/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3450 - accuracy: 0.8913\n","Epoch 498/500\n","140/140 [==============================] - 15s 106ms/step - loss: 0.3649 - accuracy: 0.8832\n","Epoch 499/500\n","140/140 [==============================] - 15s 105ms/step - loss: 0.3452 - accuracy: 0.8896\n","Epoch 500/500\n","140/140 [==============================] - 15s 104ms/step - loss: 0.3381 - accuracy: 0.8905\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fb7ab074630>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EmkaxXdjHdyd"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7RraFX9YHdye","outputId":"b932010c-14d4-4886-dd19-022ebddf0db8","executionInfo":{"status":"ok","timestamp":1588597229911,"user_tz":-330,"elapsed":8021,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print(model.evaluate(X_train, y_train, batch_size = 20))\n","print('\\nModel Performance: Log Loss and Accuracy on validation data')\n","print(model.evaluate(X_valid, y_valid, batch_size = 20))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["349/349 [==============================] - 5s 15ms/step - loss: 0.0213 - accuracy: 0.9921\n","[0.021326303482055664, 0.9921147227287292]\n","\n","Model Performance: Log Loss and Accuracy on validation data\n","117/117 [==============================] - 2s 15ms/step - loss: 28.9214 - accuracy: 0.3725\n","[28.92139434814453, 0.3724731206893921]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u5CKxykLHdyj"},"source":["## Generate text"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4JSW5EwKHdyk","outputId":"472895c8-fc53-43f4-b7e2-b56dda55cb0a","executionInfo":{"status":"ok","timestamp":1588598256409,"user_tz":-330,"elapsed":13152,"user":{"displayName":"Mansoor Rahimat Khan","photoUrl":"","userId":"09687443316707549268"}},"colab":{"base_uri":"https://localhost:8080/","height":921}},"source":["seed_length=50\n","new_words=50\n","diversity=1\n","n_gen=1\n","\n","import random\n","\n","# Choose a random sequence\n","seq = random.choice(sequences)\n","\n","# print seq\n","\n","# Choose a random starting point\n","seed_idx = random.randint(0, len(seq) - seed_length - 10)\n","# Ending index for seed\n","end_idx = seed_idx + seed_length\n","\n","gen_list = []\n","\n","for n in range(n_gen):\n","    # Extract the seed sequence\n","    seed = seq[seed_idx:end_idx]\n","    original_sequence = [idx_word[i] for i in seed]\n","    generated = seed[:] + ['#']\n","\n","    # Find the actual entire sequence\n","    actual = generated[:] + seq[end_idx:end_idx + new_words]\n","        \n","    # Keep adding new words\n","    for i in range(new_words):\n","\n","        # Make a prediction from the seed\n","        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n","\n","  \n","        # Softmax\n","        preds = preds / sum(preds)\n","\n","        # Choose the next word\n","        probas = np.random.multinomial(1, preds, 1)[0]\n","\n","        next_idx = np.argmax(probas)\n","\n","        # New seed adds on old word\n","        #             seed = seed[1:] + [next_idx]\n","        seed += [next_idx]\n","        generated.append(next_idx)\n","    # Showing generated and actual abstract\n","    n = []\n","\n","    for i in generated:\n","        n.append(idx_word.get(i, '< --- >'))\n","\n","    gen_list.append(n)\n","\n","a = []\n","\n","for i in actual:\n","    a.append(idx_word.get(i, '< --- >'))\n","\n","a = a[seed_length:]\n","\n","gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n","\n","print('Original Sequence: \\n'+' '.join(original_sequence))\n","print(\"\\n\")\n","# print(gen_list)\n","print('Generated Sequence: \\n'+' '.join(gen_list[0][1:]))\n","# print(a)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:7 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:8 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:9 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb7a6ef2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","Original Sequence: \n","pool had changed from a cup of sweet waters into a cup of salt tears they loosened the green tresses of their hair and cried to the pool and said ‘we do not wonder that you should mourn in this manner for narcissus so beautiful was he ’ ‘but was\n","\n","\n","Generated Sequence: \n","sent to her with the essay it beautiful things are listed at the moon illustration not things are listed at the market place of the market place of padua of the library of the king of burma and this volume of the text will however are are of book has\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mfKpVx8i4Y2Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}